{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tysDSWt4gK_A"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import math\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CnQip5sACa9"
   },
   "source": [
    "# Useful functions\n",
    "\n",
    "- load()\n",
    "- distance_from_rotor(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdDI1mffgaem"
   },
   "outputs": [],
   "source": [
    "def load():\n",
    "    \n",
    "    \"\"\" Load the trajectories X_1.csv, Y_1.csv and the two directional\n",
    "        aerodynamic forces FX_1.csv and FY_1.csv.\n",
    "        It translates these files into np.ndarray variables of shape (N, T)\n",
    "        where N = number of trajectories (int) and T = time steps (int) \"\"\"\n",
    "   \n",
    "    X = np.genfromtxt(\"X_1.csv\", delimiter=\",\")\n",
    "    Y = np.genfromtxt(\"Y_1.csv\", delimiter=\",\")\n",
    "    Fx = np.genfromtxt(\"FX_1.csv\", delimiter=\",\")\n",
    "    Fy = np.genfromtxt(\"FY_1.csv\", delimiter=\",\")\n",
    "    \n",
    "    \n",
    "    return X.T, Y.T, Fx.T, Fy.T\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def distance_from_rotor(X, Y):\n",
    "    \n",
    "    \"\"\" Given the two datasets containing the coordinates of all the trajectory,\n",
    "        it computes a new variable np.ndarray (N, T) containing, for all the \n",
    "        trajectories and for every time instant, the clearance parameter (i.e.\n",
    "        the distance of the center of mass of the rotor from the bearing)\n",
    "        \n",
    "        Input: X = shape(N, T), float\n",
    "               Y = shape(N, T), float\n",
    "               \n",
    "        Output: R-r = shape(N, T), float \"\"\"\n",
    "    \n",
    "    \n",
    "    R = 0.5\n",
    "    r = np.sqrt(X**2 + Y**2)\n",
    "    return R - r\n",
    "\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of the dataset\n",
    "- over 500 trajectories, the first 50 (10%) are taken as testing set, 400 (90%) are used as training set and the remaining 50 (10%) are the validation set used duering the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylv2qz_j7cU-"
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "X, Y, Fx, Fy = load()\n",
    "\n",
    "# take the first 50 trajectories (10%) that will be eventually tested\n",
    "X_to_test = X[:50, :]\n",
    "Y_to_test = Y[:50, :]\n",
    "Fx_to_test = Fx[:50, :]\n",
    "Fy_to_test = Fy[:50, :]\n",
    "dist_to_test = distance_from_rotor(X_to_test, Y_to_test)\n",
    "\n",
    "# take the rest of the trajectories (90%) to train and validate the model\n",
    "X = X[50:, :]\n",
    "Y = Y[50:, :]\n",
    "Fx = Fx[50:, :]\n",
    "Fy = Fy[50:, :]\n",
    "\n",
    "N = X.shape[0]\n",
    "D = X.shape[1]\n",
    "dist = distance_from_rotor(X, Y)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions for the creation of the dataset for CNNs\n",
    "\n",
    "Functions:\n",
    "- create_dataset_5feat_cnn\n",
    "- random_permutation\n",
    "- tensorize\n",
    "- split_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkVYOD8ZnFmI",
    "outputId": "9a8fdd49-ce76-457e-85b3-ccd81664aa60"
   },
   "outputs": [],
   "source": [
    "def create_dataset_5feat_cnn(N, D, dd, X, Y, dist, Fx, Fy):\n",
    "    \n",
    "    \"\"\" This function creates a dataset that, for each observation,\n",
    "        contains the X and Y coordinates, the clearance parameter and the forces predicted by \n",
    "        the model itself of the dd time steps prior to the time instant t the observation itself refers to.\n",
    "        Observations are matrices of dimensions (3,dd).\n",
    "        To be more clear, the dimensions of the input and the output of the funztion are:\n",
    "        \n",
    "        Input: N = scalar, int (Number of trajectories)\n",
    "               D = scalar, int (Number of overall time steps)\n",
    "               d = scalar, int (Delay parameter)\n",
    "               X = shape(N, T), float (X coordinates of all the trajectories)\n",
    "               Y = shape(N, T), float (Y coordinates of all the trajectories)\n",
    "               dist = shape(N, T), float (Clearance of all the trajectories)\n",
    "               Fx = shape(N, T), float (Aerodynamic forces along x of all the trajectories)\n",
    "               Fy = shape(N, T), float (Aerodynamic forces along y of all the trajectories)\n",
    "        \n",
    "        Output: df_input = shape(N*(D-dd), 5, dd), float (Reorganized input dataset of all the trajectories)\n",
    "                df_output = shape(N*(D-dd), 2), float (Reorganized output dataset of all the trajectories) \"\"\"\n",
    "    \n",
    "    df_input = np.zeros((N*(D-dd), 5, dd))\n",
    "    df_output = np.zeros((N*(D-dd), 2))\n",
    "    i = 0\n",
    "\n",
    "    for t in range(N):\n",
    "        \n",
    "        for n in range(D-dd):\n",
    "\n",
    "            for feat in range(5):\n",
    "\n",
    "                row = np.zeros(dd)\n",
    "                if (feat==0):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = X[t,n+d]\n",
    "              \n",
    "                elif (feat==1):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = Y[t,n+d]\n",
    "              \n",
    "                elif (feat==2):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = dist[t,n+d]\n",
    "                \n",
    "                elif (feat==3):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = Fx[t,n+d]\n",
    "\n",
    "                else: \n",
    "                    for d in range(dd):\n",
    "                        row[d] = Fy[t,n+d]\n",
    "\n",
    "                df_input[i, feat, :] = row\n",
    "          \n",
    "            df_output[i, :] = np.array([Fx[t, n+dd], Fy[t, n+dd]])\n",
    "         \n",
    "            i = i+1\n",
    "    \n",
    "    return df_input, df_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_RAFuuYBShh"
   },
   "outputs": [],
   "source": [
    "def random_permutation(df_input, df_output):\n",
    "    \n",
    "    \"\"\" Random permutation of the observations (i.e. rows) of the\n",
    "        input and output dataset.\n",
    "        R = number of rows and C = number of columns of the input\n",
    "        \n",
    "        Input: df_input = shape(R, C), float (Input dataset)\n",
    "               df_output = shape(R, 2), float (Output output)\n",
    "        \n",
    "        Output: df_in_shuff = shape(R, C), float (Shuffled input dataset)\n",
    "                df_out_shuff = shape(R, 2), float (Shuffled output dataset) \"\"\"\n",
    "    \n",
    "    \n",
    "    N = df_input.shape[0]\n",
    "    shuffle_indices = np.random.permutation(np.arange(N))\n",
    "    df_in_shuff = df_input[shuffle_indices]\n",
    "    df_out_shuff = df_output[shuffle_indices]\n",
    "\n",
    "    return df_in_shuff, df_out_shuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvsnWhIpBuSs"
   },
   "outputs": [],
   "source": [
    "def tensorize(df_in_shuff, df_out_shuff):\n",
    "    \n",
    "    \"\"\" Transform a np.ndarray into a torch.Tensor variable.\n",
    "        R = number of rows and C = number of columns of the input\n",
    "        \n",
    "        Input: df_in_shuff = shape(R, C), float (Input np.ndarray)\n",
    "               df_out_shuff = shape(R, 2), float (Output np.ndarray)\n",
    "        \n",
    "        Output: df_input_tensor = shape(R, C), float (Input torch.Tensor)\n",
    "                df_output_tensor = shape(R, 2), float (Output torch.Tensor) \"\"\"\n",
    "    \n",
    "\n",
    "    df_input_tensor = torch.Tensor(df_in_shuff)\n",
    "    df_output_tensor = torch.Tensor(df_out_shuff)\n",
    "\n",
    "    return df_input_tensor, df_output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtwtGm4qB1uW"
   },
   "outputs": [],
   "source": [
    "def split_train_val(df_input_tensor, df_output_tensor, p):\n",
    "    \n",
    "    \"\"\" Split the input dataset and the output dataset into a fraction p\n",
    "        of training set and 1-p of validation test. \n",
    "        In particular the first (100*p)% of samples are taken as training\n",
    "        and the remaining 100*(1-p)% of them is the validation set.\n",
    "        R = number of rows and C = number of columns of the input\n",
    "        \n",
    "        Input: df_input_tensor = shape(R, C), float (Input torch.Tensor)\n",
    "               df_output_tensor = shape(R, 2), float (Output torch.Tensor)\n",
    "        \n",
    "        Output: df_in_valid = shape(int(R*(1-p)), C), float (Input Validation Set torch.Tensor)\n",
    "                df_out_valid = shape(int(R*(1-p)), 2), float (Output Validation Set torch.Tensor)\n",
    "                df_in_train = shape(int(R*p), C), float (Input Training Set torch.Tensor)\n",
    "                df_out_train = shape(int(R*p), 2), float (Output Training Set torch.Tensor) \"\"\"\n",
    "  \n",
    "\n",
    "  \n",
    "    # Take the first p% of the dataset as training set and (1-p)% as validation set\n",
    "    N = df_input_tensor.shape[0]\n",
    "    df_in_train = df_input_tensor[:int(N*p), :, :]\n",
    "    df_in_valid = df_input_tensor[int(N*p):, :, :]\n",
    "\n",
    "    df_out_train = df_output_tensor[:int(N*p), :]\n",
    "    df_out_valid = df_output_tensor[int(N*p):, :]\n",
    "\n",
    "    return df_in_train, df_in_valid, df_out_train, df_out_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJe93M9_TC7D"
   },
   "outputs": [],
   "source": [
    "dd = 50\n",
    "p = 0.90\n",
    "\n",
    "df_input, df_output = create_dataset_5feat_cnn(N, D, dd, X, Y, dist, Fx, Fy)\n",
    "df_input_shuff, df_output_shuff = random_permutation(df_input, df_output)\n",
    "df_input_tensor, df_output_tensor = tensorize(df_input_shuff, df_output_shuff)\n",
    "df_in_train, df_in_valid, df_out_train, df_out_valid = split_train_val(df_input_tensor, df_output_tensor, p)\n",
    "# print(df_input.shape, df_output.shape)\n",
    "# print(df_in_train.shape, df_out_train.shape)\n",
    "# print(df_in_valid.shape, df_out_valid.shape)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(df_in_train, df_out_train)\n",
    "test = torch.utils.data.TensorDataset(df_in_valid, df_out_valid) #VALIDATION\n",
    "\n",
    "# Didn't change name not to change everything afterwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3PFJL2Bu4oI"
   },
   "source": [
    "# Aerospace Bearning 1D-Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJGz6zlKV8zS"
   },
   "outputs": [],
   "source": [
    "class Aerospace_Bearing_CNN(torch.nn.Module):\n",
    "    def __init__(self, d, feature_map1, feature_map2, feature_map3, kernel_size):\n",
    "        super().__init__()\n",
    "        self.__feature_map3 = feature_map3\n",
    "        self.__d = d\n",
    "        \n",
    "        # CASE 1: convolution to the input and 3 convolutional layers --> skip + output --> Fully connected linear\n",
    "        # Skip Connection\n",
    "        self.skipconv = torch.nn.Conv1d(in_channels=5, out_channels=feature_map3, kernel_size=kernel_size, padding='same')\n",
    "\n",
    "        # Convolution 1\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=5, out_channels=feature_map1, kernel_size=kernel_size, padding='same')\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        \n",
    "        # Convolution 2\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=feature_map1, out_channels=feature_map2, kernel_size=kernel_size, padding='same')\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        \n",
    "        # Convolution 3\n",
    "        self.conv3 = torch.nn.Conv1d(in_channels=feature_map2, out_channels=feature_map3, kernel_size=kernel_size, padding='same')\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "\n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = torch.nn.Linear(feature_map3*d, 2)\n",
    "        \n",
    "        # CASE 2: 4 convolutional layers --> skip + output --> Fully connected linear\n",
    "        # Convolution 1\n",
    "        # self.conv1 = torch.nn.Conv1d(in_channels=3, out_channels=feature_map1, kernel_size=kernel_size, padding='same')\n",
    "        # self.relu1 = torch.nn.ReLU()\n",
    "        \n",
    "        # Convolution 2\n",
    "        # self.conv2 = torch.nn.Conv1d(in_channels=feature_map1, out_channels=feature_map2, kernel_size=kernel_size, padding='same')\n",
    "        # self.relu2 = torch.nn.ReLU()\n",
    "        \n",
    "        # Convolution 3\n",
    "        # self.conv3 = torch.nn.Conv1d(in_channels=feature_map2, out_channels=feature_map3, kernel_size=kernel_size, padding='same')\n",
    "        # self.relu3 = torch.nn.ReLU()\n",
    "        \n",
    "        # Convolution 4\n",
    "        # self.conv4 = torch.nn.Conv1d(in_channels=feature_map3, out_channels=3, kernel_size=kernel_size, padding='same')\n",
    "        # self.relu4 = torch.nn.ReLU()\n",
    "        \n",
    "        # Fully connected 1 (readout)\n",
    "        # self.fc1 = torch.nn.Linear(3*d, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # CASE 1\n",
    "        skip = self.skipconv(x)\n",
    "        \n",
    "        # Convolution 1\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Convolution 2 \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Convolution 3 \n",
    "        out = self.conv3(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        # Skip connection\n",
    "        out = out + skip\n",
    "        \n",
    "        out = out.view(-1, self.__d*self.__feature_map3)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        # CASE 2\n",
    "        # skip = x\n",
    "        \n",
    "        # Convolution 1\n",
    "        # out = self.conv1(x)\n",
    "        # out = self.relu1(out)\n",
    "\n",
    "        # Convolution 2 \n",
    "        # out = self.conv2(out)\n",
    "        # out = self.relu2(out)\n",
    "        \n",
    "        # Convolution 3 \n",
    "        # out = self.conv3(out)\n",
    "        # out = self.relu3(out)\n",
    "        \n",
    "        # Convolution 4 \n",
    "        # out = self.conv4(out)\n",
    "        \n",
    "        # Skip connection\n",
    "        # out = out + skip\n",
    "        # out = self.relu4(out)\n",
    "        \n",
    "        # out = out.view(-1, self.__d*3)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        # out = self.fc1(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfT5yqghwb-P"
   },
   "source": [
    "# Training the Aerospace Bearing model with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfXs_H8Dxr98"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device):\n",
    "    \n",
    "    # Set model to training mode (affects dropout, batch norm e.g.)\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    lr_history = []\n",
    "    \n",
    "    # Change the loop to get batch_idx, data and target from train_loader\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # Move the data to the device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute model output\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform an optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Perform a learning rate scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute loss_float (float value, not a tensor)\n",
    "        loss_float = loss.item()\n",
    "\n",
    "        # Add loss_float to loss_history\n",
    "        loss_history.append(loss_float)\n",
    "\n",
    "        lr_history.append(scheduler.get_last_lr()[0])\n",
    "        if batch_idx % (len(train_loader.dataset) // len(data) // 10) == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch}-{batch_idx:03d} \"\n",
    "                f\"batch_loss={loss_float:0.2e} \"\n",
    "                # f\"batch_acc={accuracy_float:0.3f} \"\n",
    "                f\"lr={scheduler.get_last_lr()[0]:0.3e} \"\n",
    "            )\n",
    "\n",
    "    return loss_history, lr_history\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()  # Important: eval mode (affects dropout, batch norm etc)\n",
    "    test_loss = 0\n",
    "    \n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item() * len(data)\n",
    "        \n",
    "    test_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"Test set: Average loss: {:.4f}\".format(test_loss)\n",
    "    )\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_predictions(model, device, val_loader, criterion, num=None):\n",
    "    model.eval()\n",
    "    points = []\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        data = np.split(data.cpu().numpy(), len(data))\n",
    "        loss = np.split(loss.cpu().numpy(), len(data))\n",
    "        target = np.split(target.cpu().numpy(), len(data))\n",
    "        \n",
    "        points.extend(zip(data, loss, target))\n",
    "\n",
    "        if num is not None and len(points) > num:\n",
    "            break\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "def run_aerobearing_cnn_training(ddd, feature_map1, feature_map2, feature_map3, ker, num_epochs, lr, batch_size, device=\"cuda\"):\n",
    "    # ===== Data Loading =====\n",
    "    transform = transforms.ToTensor()\n",
    "    train_set = train\n",
    "    val_set = test\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # Can be important for training\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=False,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # ===== Model, Optimizer and Criterion =====\n",
    "    # d = 100\n",
    "    # feature_map1 = 16\n",
    "    # feature_map2 = 16\n",
    "    # kernel_size = 3\n",
    "    # padding = 'same'\n",
    "\n",
    "    model = Aerospace_Bearing_CNN(ddd, feature_map1, feature_map2, feature_map3, ker)\n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = torch.nn.functional.mse_loss\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader.dataset) * num_epochs) // train_loader.batch_size)\n",
    "    \n",
    "    # ===== Train Model =====\n",
    "    lr_history = []\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, lrs = train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device)\n",
    "        train_loss_history.extend(train_loss)\n",
    "        lr_history.extend(lrs)\n",
    "\n",
    "        val_loss = validate(model, device, val_loader, criterion)\n",
    "        val_loss_history.append(val_loss)\n",
    "        \n",
    "    # ===== Plot training curves =====\n",
    "    n_train = len(train_loss_history)\n",
    "    t_train = num_epochs * np.arange(n_train) / n_train\n",
    "    t_val = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(t_train, train_loss_history, label=\"Train\")\n",
    "    plt.plot(t_val, val_loss_history, label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(t_train, lr_history)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that computes the Mean Squared Error used for the iteration of the delay parameter d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse_5feat_cnn(tbp_Fx, tbp_Fy, tbp_X, tbp_Y, tbp_d, D, dd, model):\n",
    "    \n",
    "    \"\"\"COMMENT\"\"\"\n",
    "    \n",
    "    mse_vec=list()\n",
    "    \n",
    "    for m in range(len(tbp_Fx)):\n",
    "        my_Fx = tbp_Fx[m,:].copy()\n",
    "        my_Fy = tbp_Fy[m,:].copy()\n",
    "        df_tbp = np.zeros((D-dd, 5, dd))\n",
    "        df_tbp_out = np.zeros((D-dd, 2))\n",
    "            \n",
    "        row = np.zeros(dd*5)\n",
    "        for feat in range(5):\n",
    "            row = np.zeros(dd)\n",
    "        \n",
    "            if (feat==0):\n",
    "                for d in range(dd):\n",
    "                    row[d] = tbp_X[m,d]\n",
    "              \n",
    "            elif (feat==1):\n",
    "                for d in range(dd):\n",
    "                    row[d] = tbp_Y[m,d]\n",
    "              \n",
    "            elif (feat==2):\n",
    "                for d in range(dd):\n",
    "                    row[d] = tbp_d[m,d]\n",
    "                \n",
    "            elif (feat==3):\n",
    "                for d in range(dd):\n",
    "                    row[d] = tbp_Fx[m,d]\n",
    "\n",
    "            else: \n",
    "                for d in range(dd):\n",
    "                    row[d] = tbp_Fy[m,d]\n",
    "\n",
    "            df_tbp[0, feat, :] = row\n",
    "          \n",
    "        df_output[0, :] = np.array([Fx[m, dd], Fy[m, dd]])\n",
    "         \n",
    "    \n",
    "        i=1\n",
    "        pred_vec = list()\n",
    "        \n",
    "        for n in (np.arange(1, D-dd)):\n",
    "        \n",
    "            inp = torch.Tensor(df_tbp[i-1,:,:])\n",
    "            inp = torch.unsqueeze(inp,0)\n",
    "            inp = inp.to(device)\n",
    "            pred = model(inp)\n",
    "            pred = pred.to(\"cpu\")\n",
    "            pred_vec.append( [float(pred[0][0]), float(pred[0][1])])\n",
    "            my_Fx[n+dd-1] = float(pred[0][0])\n",
    "            my_Fy[n+dd-1] = float(pred[0][1])\n",
    "        \n",
    "            \n",
    "            for feat in range(5):\n",
    "                row = np.zeros(dd)\n",
    "\n",
    "                if (feat==0):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = tbp_X[m,n+d]\n",
    "\n",
    "                elif (feat==1):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = tbp_Y[m,n+d]\n",
    "\n",
    "                elif (feat==2):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = tbp_d[m,n+d]\n",
    "\n",
    "                elif (feat==3):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = my_Fx[n+d]\n",
    "\n",
    "                else: \n",
    "                    for d in range(dd):\n",
    "                        row[d] = my_Fy[n+d]\n",
    "                        \n",
    "                df_tbp[i, feat, :] = row\n",
    "\n",
    "            df_tbp_out[i, :] = np.array([tbp_Fx[m,n+dd], tbp_Fy[m,n+dd]])\n",
    "            i=i+1\n",
    "            \n",
    "        pred_vec = np.array(pred_vec)\n",
    "        mse_vec.append((np.mean((pred_vec-df_tbp_out[1:,:])**2)))\n",
    "        \n",
    "    return np.mean(mse_vec), pred_vec, df_tbp_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up of the parameters\n",
    "\n",
    "The parameters used for the iteration below are the following:\n",
    "- The minimum and maximum value, as well as the size of the step between each value attributed to the d parameter during the iteration (respectively `d_min`, `d_max`, `d_step`)\n",
    "- The learning rate, the number of feature maps and the kernel size which were obtained from the file `CNN_coord_tuning` (respectively `lr`, `feature_map1`, `feature_map2`, `feature_map3`,`ker` )\n",
    "- The size of the bacth and number of epochs (`batch_size`, `num_epochs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_min = 1\n",
    "d_max = 202\n",
    "d_step = 4\n",
    "\n",
    "p = 0.9\n",
    "\n",
    "# Set the parameters for the simulation\n",
    "lr = 0.01\n",
    "batch_size = 500\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "feature_map1 = 64\n",
    "feature_map2 = 32\n",
    "feature_map3 = 64\n",
    "ker = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute the iteration of the the training process over the delay parameter d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ZZSByD06PgX",
    "outputId": "79c950f8-7d86-4e32-c1cf-5babb3f90484"
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "nn = X_to_test.shape[0]\n",
    "\n",
    "for d in tqdm.tqdm(range(d_min, d_max, d_step)):\n",
    "    \n",
    "    df_input, df_output = create_dataset_5feat_cnn(N, D, d, X, Y, dist, Fx, Fy)\n",
    "    df_in_shuff, df_out_shuff = random_permutation(df_input, df_output)\n",
    "    df_input_tensor, df_output_tensor = tensorize(df_in_shuff, df_out_shuff)\n",
    "    df_in_train, df_in_valid, df_out_train, df_out_valid = split_train_val(df_input_tensor, df_output_tensor, p)\n",
    "    df_in_test, df_out_test = create_dataset_5feat_cnn(nn, D, d, X_to_test, Y_to_test, dist_to_test, Fx_to_test, Fy_to_test)\n",
    "    df_in_test_tensor, df_out_test_tensor = tensorize(df_in_test, df_out_test)\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(df_in_train, df_out_train)\n",
    "    test = torch.utils.data.TensorDataset(df_in_valid, df_out_valid) #VALIDATION\n",
    "    # Didn't change name not to change everything afterwords\n",
    "\n",
    "    model = run_aerobearing_cnn_training(d, feature_map1, feature_map2, feature_map3, ker, num_epochs, lr, batch_size, device)\n",
    "\n",
    "    mse = get_mse_5feat_cnn(Fx_to_test, Fy_to_test, X_to_test, Y_to_test, dist_to_test, D, d, model)[0]\n",
    "    errors.append(mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the delay parameter vs the associated error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(d_min, d_max, d_step), errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

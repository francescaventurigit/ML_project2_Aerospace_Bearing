{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tysDSWt4gK_A"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import math\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CnQip5sACa9"
   },
   "source": [
    "# Useful functions\n",
    "- load()\n",
    "- distance_from_rotor(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdDI1mffgaem"
   },
   "outputs": [],
   "source": [
    "def load():\n",
    "    \n",
    "    \"\"\" Load the trajectories X_1.csv, Y_1.csv and the two directional\n",
    "        aerodynamic forces FX_1.csv and FY_1.csv.\n",
    "        It translates these files into np.ndarray variables of shape (N, T)\n",
    "        where N = number of trajectories (int) and T = time steps (int) \"\"\"\n",
    "   \n",
    "    \n",
    "    X = np.genfromtxt(\"X_1.csv\", delimiter=\",\")\n",
    "    Y = np.genfromtxt(\"Y_1.csv\", delimiter=\",\")\n",
    "    Fx = np.genfromtxt(\"FX_1.csv\", delimiter=\",\")\n",
    "    Fy = np.genfromtxt(\"FY_1.csv\", delimiter=\",\")\n",
    "    \n",
    "    \n",
    "    return X.T, Y.T, Fx.T, Fy.T\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def distance_from_rotor(X, Y):\n",
    "    \n",
    "    \"\"\" Given the two datasets containing the coordinates of all the trajectory,\n",
    "        it computes a new variable np.ndarray (N, T) containing, for all the \n",
    "        trajectories and for every time instant, the clearance parameter (i.e.\n",
    "        the distance of the center of mass of the rotor from the bearing)\n",
    "        \n",
    "        Input: X = shape(N, T), float\n",
    "               Y = shape(N, T), float\n",
    "               \n",
    "        Output: R-r = shape(N, T), float \"\"\"\n",
    "    \n",
    "    R = 0.5\n",
    "    r = np.sqrt(X**2 + Y**2)\n",
    "    return R - r\n",
    "\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of the dataset\n",
    "- over 500 trajectories, the first 50 (10%) are taken as testing set, 400 (90%) are used as training set and the remaining 50 (10%) are the validation set used duering the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylv2qz_j7cU-"
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "X, Y, Fx, Fy = load()\n",
    "\n",
    "# take the first 50 trajectories (10%) that will be eventually tested\n",
    "X_to_test = X[:50, :]\n",
    "Y_to_test = Y[:50, :]\n",
    "Fx_to_test = Fx[:50, :]\n",
    "Fy_to_test = Fy[:50, :]\n",
    "dist_to_test = distance_from_rotor(X_to_test, Y_to_test)\n",
    "\n",
    "# take the rest of the trajectories (90%) to train and validate the model\n",
    "X = X[50:, :]\n",
    "Y = Y[50:, :]\n",
    "Fx = Fx[50:, :]\n",
    "Fy = Fy[50:, :]\n",
    "\n",
    "N = X.shape[0]\n",
    "D = X.shape[1]\n",
    "dist = distance_from_rotor(X, Y)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions for the creation of the dataset for CNNs\n",
    "\n",
    "Functions:\n",
    "- create_dataset_5feat_cnn\n",
    "- create_dataset_5feat_cnn_onetraj\n",
    "- random_permutation\n",
    "- tensorize\n",
    "- split_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkVYOD8ZnFmI",
    "outputId": "9a8fdd49-ce76-457e-85b3-ccd81664aa60"
   },
   "outputs": [],
   "source": [
    "def create_dataset_5feat_cnn(N, D, dd, X, Y, dist, Fx, Fy):\n",
    "    \n",
    "    \"\"\" This function creates a dataset that, for each observation,\n",
    "        contains the X and Y coordinates, the clearance parameter and the true forces \n",
    "        of the dd time steps prior to the time instant t the observation itself refers to.\n",
    "        Again,each observation is m aatrix of dimensions (3,dd).\n",
    "        In particular, this is done for just one trajectory (and not for all the \n",
    "        trajectories as the function above)\n",
    "        \n",
    "        Input: D = scalar, int (Number of overall time steps)\n",
    "               d = scalar, int (Delay parameter)\n",
    "               X = shape(T, ), float (X coordinates of the given trajectory)\n",
    "               Y = shape(T, ), float (Y coordinates of the given trajectory)\n",
    "               dist = shape(T, ), float (Clearance of the given trajectory)\n",
    "               Fx = shape(T, ), float (Aerodynamic forces along x of the given trajectory)\n",
    "               Fy = shape(T, ), float (Aerodynamic forces along y of the given trajectory)\n",
    "        \n",
    "        Output: df_input = shape(D-dd, 5, dd), float (Reorganized input dataset of the given trajectory)\n",
    "                df_output = shape(D-dd, 2), float (Reorganized output dataset of the given trajectory) \"\"\"\n",
    "    \n",
    "    df_input = np.zeros((N*(D-dd), 5, dd))\n",
    "    df_output = np.zeros((N*(D-dd), 2))\n",
    "    i = 0\n",
    "\n",
    "    for t in tqdm.tqdm(range(N)):\n",
    "        \n",
    "        for n in range(D-dd):\n",
    "\n",
    "            for feat in range(5):\n",
    "\n",
    "                row = np.zeros(dd)\n",
    "                if (feat==0):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = X[t,n+d]\n",
    "              \n",
    "                elif (feat==1):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = Y[t,n+d]\n",
    "              \n",
    "                elif (feat==2):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = dist[t,n+d]\n",
    "                \n",
    "                elif (feat==3):\n",
    "                    for d in range(dd):\n",
    "                        row[d] = Fx[t,n+d]\n",
    "\n",
    "                else: \n",
    "                    for d in range(dd):\n",
    "                        row[d] = Fy[t,n+d]\n",
    "\n",
    "                df_input[i, feat, :] = row\n",
    "          \n",
    "            df_output[i, :] = np.array([Fx[t, n+dd], Fy[t, n+dd]])\n",
    "         \n",
    "            i = i+1\n",
    "    \n",
    "    return df_input, df_output\n",
    "\n",
    "\n",
    "def create_dataset_5feat_cnn_onetraj(D, dd, X, Y, dist, Fx, Fy):\n",
    "    \"\"\" This function creates a dataset that, for each observation,\n",
    "        contains the X and Y coordinates, the clearance parameter and the true forces \n",
    "        of the dd time steps prior to the time instant t the observation itself refers to.\n",
    "        Again,each observation is m aatrix of dimensions (3,dd).\n",
    "        In particular, this is done for just one trajectory (and not for all the \n",
    "        trajectories as the function above)\n",
    "        \n",
    "        Input: D = scalar, int (Number of overall time steps)\n",
    "               d = scalar, int (Delay parameter)\n",
    "               X = shape(T, ), float (X coordinates of the given trajectory)\n",
    "               Y = shape(T, ), float (Y coordinates of the given trajectory)\n",
    "               dist = shape(T, ), float (Clearance of the given trajectory)\n",
    "               Fx = shape(T, ), float (Aerodynamic forces along x of the given trajectory)\n",
    "               Fy = shape(T, ), float (Aerodynamic forces along y of the given trajectory)\n",
    "        \n",
    "        Output: df_input = shape(D-dd, 5, dd), float (Reorganized input dataset of the given trajectory)\n",
    "                df_output = shape(D-dd, 2), float (Reorganized output dataset of the given trajectory) \"\"\"\n",
    "    \n",
    "    df_input = np.zeros(((D-dd), 5, dd))\n",
    "    df_output = np.zeros(((D-dd), 2))\n",
    "    i = 0\n",
    "\n",
    "    for n in range(D-dd):\n",
    "        \n",
    "        for feat in range(5):\n",
    "            \n",
    "            row = np.zeros(dd)\n",
    "            if (feat==0):\n",
    "                for d in range(dd):\n",
    "                    row[d] = X[n+d]\n",
    "              \n",
    "            elif (feat==1):\n",
    "                for d in range(dd):\n",
    "                    row[d] = Y[n+d]\n",
    "              \n",
    "            elif (feat==2):\n",
    "                for d in range(dd):\n",
    "                    row[d] = dist[n+d]\n",
    "                \n",
    "            elif (feat==3):\n",
    "                for d in range(dd):\n",
    "                    row[d] = Fx[n+d]\n",
    "\n",
    "            else: \n",
    "                for d in range(dd):\n",
    "                    row[d] = Fy[n+d]\n",
    "\n",
    "            df_input[i, feat, :] = row\n",
    "          \n",
    "        df_output[i, :] = np.array([Fx[n+dd], Fy[n+dd]])\n",
    "         \n",
    "        i = i+1\n",
    "    \n",
    "    return df_input, df_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_RAFuuYBShh"
   },
   "outputs": [],
   "source": [
    "def random_permutation(df_input, df_output):\n",
    "    \n",
    "    \"\"\" Random permutation of the observations (i.e. rows) of the\n",
    "        input and output dataset.\n",
    "        R = number of rows and C = number of columns of the input\n",
    "        \n",
    "        Input: df_input = shape(R, C), float (Input dataset)\n",
    "               df_output = shape(R, 2), float (Output output)\n",
    "        \n",
    "        Output: df_in_shuff = shape(R, C), float (Shuffled input dataset)\n",
    "                df_out_shuff = shape(R, 2), float (Shuffled output dataset) \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    N = df_input.shape[0]\n",
    "    shuffle_indices = np.random.permutation(np.arange(N))\n",
    "    df_in_shuff = df_input[shuffle_indices]\n",
    "    df_out_shuff = df_output[shuffle_indices]\n",
    "\n",
    "    return df_in_shuff, df_out_shuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvsnWhIpBuSs"
   },
   "outputs": [],
   "source": [
    "def tensorize(df_in_shuff, df_out_shuff):\n",
    "    \n",
    "    \"\"\" Transform a np.ndarray into a torch.Tensor variable.\n",
    "        R = number of rows and C = number of columns of the input\n",
    "        \n",
    "        Input: df_in_shuff = shape(R, C), float (Input np.ndarray)\n",
    "               df_out_shuff = shape(R, 2), float (Output np.ndarray)\n",
    "        \n",
    "        Output: df_input_tensor = shape(R, C), float (Input torch.Tensor)\n",
    "                df_output_tensor = shape(R, 2), float (Output torch.Tensor) \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    df_input_tensor = torch.Tensor(df_in_shuff)\n",
    "    df_output_tensor = torch.Tensor(df_out_shuff)\n",
    "\n",
    "    return df_input_tensor, df_output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtwtGm4qB1uW"
   },
   "outputs": [],
   "source": [
    "def split_train_val(df_input_tensor, df_output_tensor, p):\n",
    "    \n",
    "    \"\"\" Split the input dataset and the output dataset into a fraction p\n",
    "        of training set and 1-p of validation test. \n",
    "        In particular the first (100*p)% of samples are taken as training\n",
    "        and the remaining 100*(1-p)% of them is the validation set.\n",
    "        R = number of rows and C = number of columns of the input\n",
    "        \n",
    "        Input: df_input_tensor = shape(R, C), float (Input torch.Tensor)\n",
    "               df_output_tensor = shape(R, 2), float (Output torch.Tensor)\n",
    "        \n",
    "        Output: df_in_valid = shape(int(R*(1-p)), C), float (Input Validation Set torch.Tensor)\n",
    "                df_out_valid = shape(int(R*(1-p)), 2), float (Output Validation Set torch.Tensor)\n",
    "                df_in_train = shape(int(R*p), C), float (Input Training Set torch.Tensor)\n",
    "                df_out_train = shape(int(R*p), 2), float (Output Training Set torch.Tensor) \"\"\"\n",
    "  \n",
    "  \n",
    "  \n",
    "    # Take the first p% of the dataset as training set and (1-p)% as validation set\n",
    "    N = df_input_tensor.shape[0]\n",
    "    df_in_train = df_input_tensor[:int(N*p), :, :]\n",
    "    df_in_valid = df_input_tensor[int(N*p):, :, :]\n",
    "\n",
    "    df_out_train = df_output_tensor[:int(N*p), :]\n",
    "    df_out_valid = df_output_tensor[int(N*p):, :]\n",
    "\n",
    "    return df_in_train, df_in_valid, df_out_train, df_out_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJe93M9_TC7D"
   },
   "outputs": [],
   "source": [
    "dd = 100\n",
    "p = 0.90\n",
    "\n",
    "df_input, df_output = create_dataset_5feat_cnn(N, D, dd, X, Y, dist, Fx, Fy)\n",
    "df_input_shuff, df_output_shuff = random_permutation(df_input, df_output)\n",
    "df_input_tensor, df_output_tensor = tensorize(df_input_shuff, df_output_shuff)\n",
    "df_in_train, df_in_valid, df_out_train, df_out_valid = split_train_val(df_input_tensor, df_output_tensor, p)\n",
    "print(df_input.shape, df_output.shape)\n",
    "print(df_in_train.shape, df_out_train.shape)\n",
    "print(df_in_valid.shape, df_out_valid.shape)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(df_in_train, df_out_train)\n",
    "test = torch.utils.data.TensorDataset(df_in_valid, df_out_valid) #VALIDATION\n",
    "\n",
    "# Didn't change name not to change everything afterwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3PFJL2Bu4oI"
   },
   "source": [
    "# Aerospace Bearning 1D-Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJGz6zlKV8zS"
   },
   "outputs": [],
   "source": [
    "class Aerospace_Bearing_CNN(torch.nn.Module):\n",
    "    def __init__(self, d, feature_map1, feature_map2, feature_map3, kernel_size):\n",
    "        super().__init__()\n",
    "        self.__feature_map3 = feature_map3\n",
    "        self.__d = d\n",
    "        \n",
    "        # CASE 1: convolution to the input and 3 convolutional layers --> skip + output --> Fully connected linear\n",
    "        # Skip Connection\n",
    "        self.skipconv = torch.nn.Conv1d(in_channels=5, out_channels=feature_map3, kernel_size=kernel_size, padding='same')\n",
    "\n",
    "        # Convolution 1\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=5, out_channels=feature_map1, kernel_size=kernel_size, padding='same')\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        \n",
    "        # Convolution 2\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=feature_map1, out_channels=feature_map2, kernel_size=kernel_size, padding='same')\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        \n",
    "        # Convolution 3\n",
    "        self.conv3 = torch.nn.Conv1d(in_channels=feature_map2, out_channels=feature_map3, kernel_size=kernel_size, padding='same')\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "\n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = torch.nn.Linear(feature_map3*d, 2)\n",
    "        \n",
    "        # CASE 2: 4 convolutional layers --> skip + output --> Fully connected linear\n",
    "        # Convolution 1\n",
    "        # self.conv1 = torch.nn.Conv1d(in_channels=3, out_channels=feature_map1, kernel_size=kernel_size, padding='same')\n",
    "        # self.relu1 = torch.nn.ReLU()\n",
    "        \n",
    "        # Convolution 2\n",
    "        # self.conv2 = torch.nn.Conv1d(in_channels=feature_map1, out_channels=feature_map2, kernel_size=kernel_size, padding='same')\n",
    "        # self.relu2 = torch.nn.ReLU()\n",
    "        \n",
    "        # Convolution 3\n",
    "        # self.conv3 = torch.nn.Conv1d(in_channels=feature_map2, out_channels=feature_map3, kernel_size=kernel_size, padding='same')\n",
    "        # self.relu3 = torch.nn.ReLU()\n",
    "        \n",
    "        # Convolution 4\n",
    "        # self.conv4 = torch.nn.Conv1d(in_channels=feature_map3, out_channels=3, kernel_size=kernel_size, padding='same')\n",
    "        # self.relu4 = torch.nn.ReLU()\n",
    "        \n",
    "        # Fully connected 1 (readout)\n",
    "        # self.fc1 = torch.nn.Linear(3*d, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # CASE 1\n",
    "        skip = self.skipconv(x)\n",
    "        \n",
    "        # Convolution 1\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Convolution 2 \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Convolution 3 \n",
    "        out = self.conv3(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        # Skip connection\n",
    "        out = out + skip\n",
    "        \n",
    "        out = out.view(-1, self.__d*self.__feature_map3)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        # CASE 2\n",
    "        # skip = x\n",
    "        \n",
    "        # Convolution 1\n",
    "        # out = self.conv1(x)\n",
    "        # out = self.relu1(out)\n",
    "\n",
    "        # Convolution 2 \n",
    "        # out = self.conv2(out)\n",
    "        # out = self.relu2(out)\n",
    "        \n",
    "        # Convolution 3 \n",
    "        # out = self.conv3(out)\n",
    "        # out = self.relu3(out)\n",
    "        \n",
    "        # Convolution 4 \n",
    "        # out = self.conv4(out)\n",
    "        \n",
    "        # Skip connection\n",
    "        # out = out + skip\n",
    "        # out = self.relu4(out)\n",
    "        \n",
    "        # out = out.view(-1, self.__d*3)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        # out = self.fc1(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfT5yqghwb-P"
   },
   "source": [
    "# Training the Aerospace Bearing model with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfXs_H8Dxr98"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device):\n",
    "    \n",
    "    # Set model to training mode (affects dropout, batch norm e.g.)\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    lr_history = []\n",
    "    \n",
    "    # Change the loop to get batch_idx, data and target from train_loader\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # Move the data to the device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute model output\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform an optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Perform a learning rate scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute loss_float (float value, not a tensor)\n",
    "        loss_float = loss.item()\n",
    "\n",
    "        # Add loss_float to loss_history\n",
    "        loss_history.append(loss_float)\n",
    "\n",
    "        lr_history.append(scheduler.get_last_lr()[0])\n",
    "        if batch_idx % (len(train_loader.dataset) // len(data) // 10) == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch}-{batch_idx:03d} \"\n",
    "                f\"batch_loss={loss_float:0.2e} \"\n",
    "                # f\"batch_acc={accuracy_float:0.3f} \"\n",
    "                f\"lr={scheduler.get_last_lr()[0]:0.3e} \"\n",
    "            )\n",
    "\n",
    "    return loss_history, lr_history\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()  # Important: eval mode (affects dropout, batch norm etc)\n",
    "    test_loss = 0\n",
    "    \n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item() * len(data)\n",
    "        \n",
    "    test_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"Test set: Average loss: {:.4f}\".format(test_loss)\n",
    "    )\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_predictions(model, device, val_loader, criterion, num=None):\n",
    "    model.eval()\n",
    "    points = []\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        data = np.split(data.cpu().numpy(), len(data))\n",
    "        loss = np.split(loss.cpu().numpy(), len(data))\n",
    "        target = np.split(target.cpu().numpy(), len(data))\n",
    "        \n",
    "        points.extend(zip(data, loss, target))\n",
    "\n",
    "        if num is not None and len(points) > num:\n",
    "            break\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "def run_aerobearing_cnn_training(ddd, feature_map1, feature_map2, feature_map3, ker, num_epochs, lr, batch_size, device=\"cuda\"):\n",
    "    # ===== Data Loading =====\n",
    "    transform = transforms.ToTensor()\n",
    "    train_set = train\n",
    "    val_set = test\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # Can be important for training\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=False,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # ===== Model, Optimizer and Criterion =====\n",
    "    # d = 100\n",
    "    # feature_map1 = 16\n",
    "    # feature_map2 = 16\n",
    "    # kernel_size = 3\n",
    "    # padding = 'same'\n",
    "\n",
    "    model = Aerospace_Bearing_CNN(ddd, feature_map1, feature_map2, feature_map3, ker)\n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = torch.nn.functional.mse_loss\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader.dataset) * num_epochs) // train_loader.batch_size)\n",
    "    \n",
    "    # ===== Train Model =====\n",
    "    lr_history = []\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, lrs = train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device)\n",
    "        train_loss_history.extend(train_loss)\n",
    "        lr_history.extend(lrs)\n",
    "\n",
    "        val_loss = validate(model, device, val_loader, criterion)\n",
    "        val_loss_history.append(val_loss)\n",
    "        \n",
    "    # ===== Plot training curves =====\n",
    "    n_train = len(train_loss_history)\n",
    "    t_train = num_epochs * np.arange(n_train) / n_train\n",
    "    t_val = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(t_train, train_loss_history, label=\"Train\")\n",
    "    plt.plot(t_val, val_loss_history, label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(t_train, lr_history)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up of the parameters\n",
    "\n",
    "The parameters which characterize the model are the following:\n",
    "- The learning rate and the number of hidden layers which were obtained from the file `CNN_predicted_tuning` (respectively `lr`, `feature_map1`, `feature_map2`, `feature_map3`, `ker`)\n",
    "- The size of the bacth and number of epochs (`batch_size`, `num_epochs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "id": "_EsG1D9hywF7",
    "outputId": "919668bc-d19a-49a3-d059-ce48f1ec502f"
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "batch_size = 500\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ddd = 100\n",
    "feature_map1 = 64\n",
    "feature_map2 = 32\n",
    "feature_map3 = 64\n",
    "ker = 3\n",
    "\n",
    "model = run_aerobearing_cnn_training(ddd, feature_map1, feature_map2, feature_map3, ker, num_epochs, lr, batch_size, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MSE(df_in_test_tensor, df_out_test_tensor):\n",
    "    \n",
    "    losses = []\n",
    "    for i in tqdm.tqdm(range(len(df_in_test_tensor))):\n",
    "        \n",
    "        row = df_in_test_tensor[i, :, :]\n",
    "        \n",
    "        pred = model(row.to(device))\n",
    "        pred = pred.to('cpu')\n",
    "        \n",
    "        loss = float(torch.mean((df_out_test_tensor[i, :]-pred)**2))\n",
    "        losses.append(loss)\n",
    "    \n",
    "    mse = np.mean(losses)\n",
    "\n",
    "    return mse, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RE(df_in_test_tensor, df_out_test_tensor):\n",
    "    \n",
    "    losses = []\n",
    "    for i in tqdm.tqdm(range(len(df_in_test_tensor))):\n",
    "        \n",
    "        row = df_in_test_tensor[i, :, :]\n",
    "        \n",
    "        pred = model(row.to(device))\n",
    "        pred = pred.to('cpu')\n",
    "        \n",
    "        # RMSE of the relative error\n",
    "        loss = float(torch.mean((torch.abs((df_out_test_tensor[i, :] - pred)/df_out_test_tensor[i, :]))))\n",
    "        losses.append(loss)\n",
    "    \n",
    "    losses = np.asarray(losses)\n",
    "    re = np.mean(losses)\n",
    "\n",
    "    return re, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataset to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ZZSByD06PgX",
    "outputId": "79c950f8-7d86-4e32-c1cf-5babb3f90484"
   },
   "outputs": [],
   "source": [
    "# create the dataset to test\n",
    "nn = X_to_test.shape[0]\n",
    "df_in_test, df_out_test = create_dataset_5feat_cnn(nn, D, dd, X_to_test, Y_to_test, dist_to_test, Fx_to_test, Fy_to_test)\n",
    "df_in_test_tensor, df_out_test_tensor = tensorize(df_in_test, df_out_test)\n",
    "\n",
    "mse, losses = get_MSE(df_in_test_tensor, df_out_test_tensor)\n",
    "err_rel, losses_rel = get_RE(df_in_test_tensor, df_out_test_tensor)\n",
    "print(mse)\n",
    "print(err_rel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of a given trajectory of aerodynamic forces and its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory_true_cnn(dd, traj):\n",
    "    \n",
    "    # save one trajectory to be plotted (tbp)\n",
    "    tbp_X = X_to_test[traj,:]\n",
    "    tbp_Y = Y_to_test[traj,:]\n",
    "    tbp_Fx = Fx_to_test[traj,:]\n",
    "    tbp_Fy = Fy_to_test[traj,:]\n",
    "    tbp_d = dist_to_test[traj,:]\n",
    "    \n",
    "    traj_in, traj_out = create_dataset_5feat_cnn_onetraj(D, dd, tbp_X, tbp_Y, tbp_d, tbp_Fx, tbp_Fy)\n",
    "    \n",
    "    fx_pred_tbp = []\n",
    "    fy_pred_tbp = []\n",
    "\n",
    "    for i in range(len(traj_in)):\n",
    "        row = torch.Tensor(traj_in[i,:,:])\n",
    "        # row = torch.unsqueeze(row,0)\n",
    "        pred = model(row.to(device))\n",
    "        fx_pred_tbp.append(float(pred[0][0]))\n",
    "        fy_pred_tbp.append(float(pred[0][1]))\n",
    "\n",
    "\n",
    "    plt.plot(fx_pred_tbp, fy_pred_tbp, label=\"Pred\")  \n",
    "    plt.plot(traj_out[:,0], traj_out[:,1], label=\"True\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = 40\n",
    "plot_trajectory_true_cnn(dd, traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = np.array([1.7061752324914034e-06, 9.666283316614332e-07, 1.1066054877145272e-06, 1.6341525158672188e-06, 1.301171270085897e-06])\n",
    "RE = np.array([0.007834065234536681, 0.005773095104730237, 0.009195403920162786, 0.011240532090610163, 0.00656441371849933])\n",
    "\n",
    "mean_MSE = np.mean(MSE)\n",
    "std_MSE = np.std(MSE)\n",
    "mean_RE = np.mean(RE)\n",
    "std_RE = np.std(RE)\n",
    "\n",
    "CI_MSE = 1.96*std_MSE\n",
    "CI_RE = 1.96*std_RE\n",
    "\n",
    "print(mean_MSE, CI_MSE)\n",
    "print(mean_RE, CI_RE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
